from google.cloud import bigquery
import requests
import pandas
import json

def main():
  get_data_from_api()
  write_to_csv()
  load_to_bq()

  print("All steps completed")

def get_data_from_api():
  url = "https://api.dataforseo.com/v3/serp/google/organic/live/advanced"

  payload="[{\"keyword\":\"bensons for beds\", \"location_code\":2826, \"language_code\":\"en\", \"device\":\"desktop\", \"os\":\"windows\", \"depth\":10, \"search_param\":\"adtest=on\"}]"
  headers = {
    'Authorization': 'Basic YW15LmNoYW5AdGhlN3N0YXJzLmNvLnVrOlkwdFRiZWdpbg==',
    'Content-Type': 'application/json'
  }

  response = requests.request("GET", url, headers=headers, data=payload)
  print(response.text)
  myjson = response.json()
  print("step 1/3 - successful)

def write_to_csv():
  data = get_data_from_api()
  data_csv = pandas.DataFrame(data)
  data_csv.to_csv("INSERT FILE ROUTE")
  print("step 2/3 - successful")

def load_to_bq():
  client = bigquery.Client.from_service_account_json(
    "DEFINE USER ACCOUNT HERE.json"
  )
  
  table_id = "PATH TO TABLE WANT TO LOAD"

  job_config = bigquery.LoadJobConfig(
    source_format = bigquery.SourceFormat.CSV, skip_leading_rows = 1, autodetect = TRUE
  )

  with open("PATH TO TABLE WANT TO LOAD", "rb") as source_file:
    job = client.load_table_from_file(source_file, table_id, job_config = job_config)

    job.result()
    table = client.get_table(table_id)

    print("step 3/3 - successful - {} rows to table {}". format(table.num_rows, table_id))
